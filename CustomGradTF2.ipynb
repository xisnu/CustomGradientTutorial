{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomGradTF2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6iyqj65B6bwnXlSD/piN+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xisnu/CustomGradientTutorial/blob/master/CustomGradTF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8gsf4Nn3gm-",
        "colab_type": "text"
      },
      "source": [
        "Import Tensorflow 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_5UpAGd3V0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqenObzS3pb4",
        "colab_type": "text"
      },
      "source": [
        "We are trying to compute the following function in forward pass\n",
        "\n",
        "$y=5x^2$ + 3\n",
        "\n",
        "But we want to do it in two steps,\n",
        "\n",
        "1. $y_{1}=x^{2}$\n",
        "1. $y_{2}=5y_{1} + 3$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTlUgi0_3fFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e58a449f-d62c-4fd9-ad96-8e25fe4a5ec2"
      },
      "source": [
        "def step_1(x):\n",
        "  return x**2\n",
        "\n",
        "def step_2(x):\n",
        "  return 5*x + 3\n",
        "\n",
        "# Test them\n",
        "x = tf.constant(6.0) # y = 5 * 6^2 + 3 = 183\n",
        "y1 = step_1(x)\n",
        "print(\"Step 1 Output\",y1.numpy())\n",
        "y2 = step_2(y1)\n",
        "print(\"Step 2 Output\",y2.numpy())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 Output 36.0\n",
            "Step 2 Output 183.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUkPvOoh6Xxw",
        "colab_type": "text"
      },
      "source": [
        "Now let's find the derivatives from each step\n",
        "\n",
        "1. $\\frac{dy_{1}}{dx} = 2x$\n",
        "1. $\\frac{dy_{2}}{dx} = 10x$\n",
        "\n",
        "Test what TensorFlow computes for step 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-NUTBq37QmL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b90185a3-cdfa-4eb2-fab6-1ea09278901d"
      },
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y1 = step_1(x)\n",
        "  y2 = step_2(y1)\n",
        "  grad = tape.gradient(y1,x)\n",
        "print(\"Gradient for step 1\",grad.numpy()) # should be 2x"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient for step 1 12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baYxQgdp74tv",
        "colab_type": "text"
      },
      "source": [
        "Test gradient for step 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7oC5EPz771W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6bc9511d-a282-4793-8859-531b58b262d0"
      },
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y1 = step_1(x)\n",
        "  y2 = step_2(y1)\n",
        "  grad = tape.gradient(y2,x)\n",
        "print(\"Gradient for step 1\",grad.numpy()) # should be 10x"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient for step 1 60.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpSRUdR28Yq9",
        "colab_type": "text"
      },
      "source": [
        "So for standard functions TF computes both the gradients perfectly. But what happens if one of the functions is not differentiable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXbSWd6x8rpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bad_step_1(x): # Not Differentiable everywhere\n",
        "  if(x>5):\n",
        "    return x**2\n",
        "  else:\n",
        "    return tf.constant(0.0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwGNnLeM_Mwb",
        "colab_type": "text"
      },
      "source": [
        "This is something like RELU\n",
        "\n",
        "Now,\n",
        "\n",
        "$y_{2} = 5x^{2} + 3 \\quad if \\;  x \\gt 5 $\n",
        "\n",
        "$y_{2} = 3 \\quad otherwise $\n",
        "\n",
        "Note that the left and right derivative at $x=5$ is different, therefore this function is not differentiable at $x=5$\n",
        "\n",
        "First, test with $x>5$, \n",
        "\n",
        "the derivatives are $\\frac{dy_{1}}{dx}=2x$ and $\\frac{dy_{2}}{dx}=10x$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_QyTUGmA2O1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e11252d0-4eb7-4dbf-f2eb-6be2a9a4080c"
      },
      "source": [
        "# for x > 5\n",
        "x = tf.constant(7.0) # y = 5 * 7^2 + 3 = 248\n",
        "with tf.GradientTape() as tape1,tf.GradientTape() as tape2:\n",
        "  tape1.watch(x)\n",
        "  tape2.watch(x)\n",
        "  y1 = bad_step_1(x)\n",
        "  y2 = step_2(y1)\n",
        "  grad1 = tape1.gradient(y1,x)\n",
        "  grad2 = tape2.gradient(y2,x)\n",
        "print(\"First step output \",y1.numpy())\n",
        "print(\"Second step output \",y2.numpy())\n",
        "print(\"Gradient for Step 1 \",grad1.numpy())\n",
        "print(\"Gradient for Step 2 \",grad2.numpy())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First step output  49.0\n",
            "Second step output  248.0\n",
            "Gradient for Step 1  14.0\n",
            "Gradient for Step 2  70.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7HgsbIL-5d9",
        "colab_type": "text"
      },
      "source": [
        "and for $x\\le5$\n",
        "\n",
        "Automatic differentiation is not possible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHxZek-j-9Ym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5b992439-b7f2-4ab1-a3d8-ff58ccfc47ae"
      },
      "source": [
        "# for x < 5\n",
        "x = tf.constant(2.0) # y = 5 * 0 + 3 = 3\n",
        "with tf.GradientTape() as tape1,tf.GradientTape() as tape2:\n",
        "  tape1.watch(x)\n",
        "  tape2.watch(x)\n",
        "  y1 = bad_step_1(x)\n",
        "  y2 = step_2(y1)\n",
        "  grad1 = tape1.gradient(y1,x)\n",
        "  grad2 = tape2.gradient(y2,x)\n",
        "print(\"First step output \",y1.numpy())\n",
        "print(\"Second step output \",y2.numpy())\n",
        "print(\"Gradient for Step 1 \",grad1)\n",
        "print(\"Gradient for Step 2 \",grad2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First step output  0.0\n",
            "Second step output  3.0\n",
            "Gradient for Step 1  None\n",
            "Gradient for Step 2  None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbQuERc6Excd",
        "colab_type": "text"
      },
      "source": [
        "This is a problem as sometimes in our forward pass we need to include operations that are not differentiable in backpropagation. We need to define our custom gradient for those functions. This computation is defined by us. In this example \n",
        "\n",
        "$\\frac{dy_{1}}{dx}=2x \\quad if \\;\\; x\\gt5$ and\n",
        "\n",
        "$\\frac{dy_{1}}{dx}=1 \\quad otherwise$\n",
        "\n",
        "This is not true in true mathematical sense. We are trying to pass `incoming` gradients **Straight Through**, this apporoach is similar to **Straight Through Estimator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9blsu5I5FJUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.custom_gradient\n",
        "def bad_step_grad(x): # custom gradient everywhere\n",
        "  f = bad_step_1(x) # this is what the function will do in forward pass\n",
        "  def custom_grad(dy):\n",
        "    if(x>5):\n",
        "      g = 2*x * dy # what automatic differentiation does\n",
        "    else:\n",
        "      g = dy # this is waht we are defining\n",
        "    return g\n",
        "  return f,custom_grad"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "595EnphdIsSm",
        "colab_type": "text"
      },
      "source": [
        "Time to test our Custom Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osb7CMB6Ixrg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4d1b8191-d9ca-4fd4-e574-7f0757a643e8"
      },
      "source": [
        "# for x < 5\n",
        "x = tf.constant(2.0) # y = 5 * 0 + 3 = 3\n",
        "with tf.GradientTape() as tape1,tf.GradientTape() as tape2:\n",
        "  tape1.watch(x)\n",
        "  tape2.watch(x)\n",
        "  y1 = bad_step_grad(x)\n",
        "  y2 = step_2(y1)\n",
        "  grad1 = tape1.gradient(y1,x)\n",
        "  grad2 = tape2.gradient(y2,x)\n",
        "print(\"First step output \",y1.numpy())\n",
        "print(\"Second step output \",y2.numpy())\n",
        "print(\"Gradient for Step 1 \",grad1.numpy())\n",
        "print(\"Gradient for Step 2 \",grad2.numpy())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First step output  0.0\n",
            "Second step output  3.0\n",
            "Gradient for Step 1  1.0\n",
            "Gradient for Step 2  5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "latyyiZuJkOh",
        "colab_type": "text"
      },
      "source": [
        "For $x\\gt5$ this will behave like automatic gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qXyOWShJOf1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a55313e4-8995-4e1e-e97f-fe52475fdf3a"
      },
      "source": [
        "# for x > 5\n",
        "x = tf.constant(7.0) # y = 5 * 7^2 + 3 = 248\n",
        "with tf.GradientTape() as tape1,tf.GradientTape() as tape2:\n",
        "  tape1.watch(x)\n",
        "  tape2.watch(x)\n",
        "  y1 = bad_step_grad(x)\n",
        "  y2 = step_2(y1)\n",
        "  grad1 = tape1.gradient(y1,x)\n",
        "  grad2 = tape2.gradient(y2,x)\n",
        "print(\"First step output \",y1.numpy())\n",
        "print(\"Second step output \",y2.numpy())\n",
        "print(\"Gradient for Step 1 \",grad1.numpy())\n",
        "print(\"Gradient for Step 2 \",grad2.numpy())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First step output  49.0\n",
            "Second step output  248.0\n",
            "Gradient for Step 1  14.0\n",
            "Gradient for Step 2  70.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWAPKTvnKI6I",
        "colab_type": "text"
      },
      "source": [
        "This demonstration gives us a good insight of gradient computaion in TF2, and also illustrates the differentiability of **RELU** and **Straight Through Estimator**"
      ]
    }
  ]
}